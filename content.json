{"meta":{"title":"Sky`s Blog","subtitle":"","description":"以梦为马，不负韶华","author":"Sky","url":"https://www.flamingo-sky.com","root":"/"},"pages":[{"title":"","date":"2022-02-12T18:56:54.447Z","updated":"2022-02-12T18:56:54.447Z","comments":false,"path":"about/index.html","permalink":"https://www.flamingo-sky.com/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"404 Not Found：该页无法显示","date":"2022-02-10T18:07:10.137Z","updated":"2022-01-03T09:09:36.000Z","comments":false,"path":"/404.html","permalink":"https://www.flamingo-sky.com/404.html","excerpt":"","text":""},{"title":"书单","date":"2022-02-10T17:00:52.587Z","updated":"2022-01-03T09:09:36.000Z","comments":false,"path":"books/index.html","permalink":"https://www.flamingo-sky.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-02-12T15:57:51.323Z","updated":"2022-02-12T15:57:51.323Z","comments":false,"path":"categories/index.html","permalink":"https://www.flamingo-sky.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-02-10T17:00:52.591Z","updated":"2022-01-03T09:09:36.000Z","comments":true,"path":"links/index.html","permalink":"https://www.flamingo-sky.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-02-10T17:00:52.594Z","updated":"2022-01-03T09:09:36.000Z","comments":false,"path":"repository/index.html","permalink":"https://www.flamingo-sky.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-02-12T15:57:58.854Z","updated":"2022-02-12T15:57:58.854Z","comments":false,"path":"tags/index.html","permalink":"https://www.flamingo-sky.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Zookeeper协议、服务器角色及启动、leader选举","slug":"Zookeeper协议、服务器角色及启动、leader选举","date":"2022-02-14T22:56:54.000Z","updated":"2022-02-14T23:06:52.906Z","comments":true,"path":"2022/02/15/Zookeeper协议、服务器角色及启动、leader选举/","link":"","permalink":"https://www.flamingo-sky.com/2022/02/15/Zookeeper%E5%8D%8F%E8%AE%AE%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A7%92%E8%89%B2%E5%8F%8A%E5%90%AF%E5%8A%A8%E3%80%81leader%E9%80%89%E4%B8%BE/","excerpt":"","text":"Zookeeper协议、服务器角色及启动、leader选举一.ZAB协议1.概念Zookeeper是使用的Zookeeper Atomic Broadcast（ZAB，Zookeeper原子消息广播协议）协议作为数据一致性的核心算法，并没有完全采用paxos算法。ZAB协议是一种专门为Zookeeper设计的支持崩溃恢复的原子广播协议。 基于该协议，Zookeeper实现了一种主备模式的系统架构来保持集群中各副本之间的数据一致性。 ZAB协议的核心是定义了对于那些会改变Zookeeper服务器数据状态的事务请求的处理方式。 即所有事务请求必须由一个全局唯一的服务器来协调处理,这样的服务器被称为Leader服务器,余下的服务器则称为Follower服务器, Leader服务器负责将一个客户端事务请求转化成一个事务Proposal (提·议) ,并将该Proposal分发给集群中所有的Follower服务器,之后Leader服务器需要等待所有Follower服务器的反馈,一旦超过半数的Follower服务器进行了正确的反馈后,那么Leader就会再次向所有的Follower服务器分发Commit消息,要求其将前一个Proposal进行提交。 2.ZAB协议两种基本模式ZAB协议包括两种基本模式：崩溃恢复和消息广播。 进入崩溃恢复模式： 当整个服务框架启动过程中,或者是Leader服务器出现网络中断、崩溃退出或重启等异常情况时, ZAB协议就会进入崩溃恢复模式,同时选举产生新的Leader服务器。当选举产生了新的Leader服务器,同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后, ZAB协议就会退出恢复模式,其中,所谓的状态同步就是指数据同步,用来保证集群中过半的机器能够和Leader服务器的数据状态保持一致。 进入消息广播模式： 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步,那么整个服务框架就可以进入消息广播模式,当一台同样遵守ZAB协议的服务器启动后加入到集群中,如果此时集群中已经存在一个Leader服务器在负责进行消息广播,那么加入的服务器就会自觉地进入数据恢复模式:找到Leader所在的服务器,并与其进行数据同步,然后一起参与到消息广播流程中去。Zookeeper只允许唯一的一个Leader服务器来进行事务请求的处理, Leader服务器在接收到客户端的事务请求后,会生成对应的事务提议并发起一轮广播协议,而如果集群中的其他机器收到客户端的事务请求后,那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 2.1 消息广播ZAB协议使用原子广播协议，类似于一个二阶段提交过程，对于客户端的事务请求，Leader服务器会为其生成对应的事务Proposal,并将其发送给集群中其余所有的机器,然后再分别收集各自的选票,最后进行事务提交。 二阶段提交过程中，移出了中断逻辑，即在过半的Follower服务器已经反馈Ack后就可以开始提交事务Proposal了； ZAB采用崩溃恢复模式来解决因Leader服务器崩溃退出而带来的数据不一致性问题； 整个消息广播协议是基于具有FIFO特性的TCP协议来进行网络通信的，可以保证消息广播过程中消息接受与发送的顺序性。 在整个消息广播过程中, Leader服务器会为每个事务请求生成对应的Proposal来进行广播,并且在广播事务Proposal之前, Leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID,称之为事务ID (ZXID) ,由于ZAB协议需要保证每个消息严格的因果关系,因此必须将每个事务Proposal按照其ZXID的先后顺序来进行排序和处理。 具体的过程：在消息广播过程中, Leader服务器会为每一个Follower服务器都各自分配一个单独的队列,然后将需要广播的事务Proposal依次放入这些队列中去,并且根据FIFO策略进行消息发送。每一个Follower服务器在接收到这个事务Proposal之后,都会首先将其以事务日志的形式写入到本地磁盘中去,并且在成功写入后反馈给Leader服务器一个Ack响应。当Leader服务器接收到超过半数Follower的Ack响应后,就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交,同时Leader自身也会完成对事务的提交,而每一个Follower服务器在接收到Commit消息后,也会完成对事务的提交。 2.2 崩溃恢复一旦在Leader服务器出现崩溃，或者由于网络原因导致Leader服务器失去了过半Follower的联系，就会进入崩溃恢复模式。整个恢复过程结束后需要选举一个新的Leader服务器，因此，ZAB协议就需要一个高效且可靠的Leader选举算法来保证快速的选出新的Leader。 基本特性 ZAB协议规定了如果一个事务Proposal在一台机器上被处理成功,那么应该在所有的机器上都被处理成功,哪怕机器出现故障崩溃。 ZAB协议需要确保那些已经在Leader服务器上提交的事务最终被所有服务器都提交; ZAB协议需要确保丢弃那些只在Leader服务器上被提出的事务。 ZAB协议必须设计这样一个Leader选举算法：能够确保提交已经被Leader提交的事务Proposal,同时丢弃已经被跳过的事务Proposal。针对这个要求,如果让Leader选举算法能够保证新选举出来的Leader服务器拥有集群中所有机器最高编号(即ZXID最大)的事务Proposal,那么就可以保证这个新选举出来的Leader一定具有所有已经提交的提案。如果让具有最高编号事务Proposal的机器来成为Leader,就可以省去Leader服务器检查Proposal的提交和丢弃工作的这一步操作了。 数据同步 完成Leader选举之后,在正式开始工作(即接收客户端的事务请求,然后提出新的提案)之前,Leader服务器会首先确认事务日志中的所有Proposal是否都已经被集群中过半的机器提交了,即是否完成数据同步。下面我们就来看看ZAB协议的数据同步过程。 所有正常运行的服务器,要么成为Leader,要么成为Follower并和Leader保持同步。Leader服务器需要确保所有的Follower服务器能够接收到每一条事务Proposal,并且能够正确地将所有已经提交了的事务Proposal应用到内存数据库中去。具体的, Leader服务器会为每一个Follower服务器都准备一个队列,并将那些没有被各Follower服务器同步的事务以Proposal消息的形式逐个发送给Follower服务器,并在每一个Proposal消息后面紧接着再发送一个Commit消息,以表示该事务已经被提交。等到Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后, Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中,并开始之后的其他流程。 3.运行时状态分析在ZAB协议的设计中，每个进程都有可能处于下面三种状态之一： Looking：Leader选举阶段； Following：Follower服务器和Leader服务器保持同步状态； Leading：Leader服务器作为主进程领导状态。 所有进程初始状态都是LOOKING状态,此时不存在Leader,接下来,进程会试图选举出一个新的Leader,之后,如果进程发现已经选举出新的Leader了,那么它就会切换到FOLLOWING状态,并开始和Leader保持同步,处于FOLLOWING状态的进程称为Follower, LEADING状态的进程称为Leader,当Leader崩溃或放弃领导地位时,其余的Follower进程就会转换到LOOKING状态开始新一轮的Leader选举。 一个Follower只能和一个Leader保持同步, Leader进程和所有的Follower进程之间都通过心跳检测机制来感知彼此的情况。若Leader能够在超时时间内正常收到心跳检测,那么Follower就会一直与该eader保持连接,而如果在指定时间内Leader无法从过半的Follower进程那里接收到心跳检测,或者TCP连接断开,那么Leader会放弃当前周期的领导,并转换到LOOKING状态,其他的Follower也会选择放弃这个Leader,同时转换到LOOKING状态,之后会进行新一轮的Leader选举。 4.ZAB与Paxos联系和区别联系： 都存在一个类似于Leader进程的角色,由其负责协调多个Follower进程的运行。 Leader进程都会等待超过半数的Follower做出正确的反馈后,才会将一个提议进行提交。 在ZAB协议中,每个Proposal中都包含了一个epoch值,用来代表当前的Leader周期,在Paxos算法中,同样存在这样的一个标识,名字为Ballot。 区别： Paxos算法中,新选举产生的主进程会进行两个阶段的工作,第一阶段称为读阶段,新的主进程和其他进程通信来收集主进程提出的提议,并将它们提交。第二阶段称为写阶段,当前主进程开始提出自己的提议。 ZAB协议在Paxos基础上添加了同步阶段,此时,新的Leader会确保存在过半的Follower已经提交了之前的Leader周期中的所有事务Proposal,这一同步阶段的引入,能够有效地保证Leader在新的周期中提出事务Proposal之前,所有的进程都已经完成了对之前所有事务Proposal的提交。 总的来说, ZAB协议和Paxos算法的本质区别在于,两者的设计目标不太一样, ZAB协议主要用于构建一个高可用的分布式数据主备系统,而Paxos算法则用于构建一个分布式的一致性状态机系统。 二.服务器角色1.LeaderLeader服务器是Zookeeper集群工作的核心，其主要工作有以下两个： 事务请求的唯一调度和处理者,保证集群事务处理的顺序性。 集群内部各服务器的调度者。 使用责任链来处理每个客户端的请求是Zookeeper的特色, Leader服务器的请求处理链如下: 可以看到,从prepRequestProcessor到FinalRequestProcessor前后一共7个请求处理器组成了leader服务器的请求处理链。 PrepRequestProcessor,请求预处理器。也是leader服务器中的第一个请求处理器。在Zookeeper中,那些会改变服务器状态的请求称为事务请求(创建节点、更新数据、删除节点、创建会话等). PrepRequestProcessor能够识别出当前客户端请求是否是事务请求。对于事务请求,PrepRequestProcessor处理器会对其进行一系列预处理,如创建请求事务头、事务体、会话检查、ACL检查和版本检查等。 ProposalRequestProcessor,事务投票处理器。也是Leader服务器事务处理流程的发起者,对于非事务性请求, ProposalRequestProcessor会直接将请求转发到CommitProcessor处理器,不再做任何处理,而对于事务性请求,处理将请求转发到CommitProcessor外,还会根据请求类型创建对应的Proposal提议,并发送给所有的Follower服务器来发起一次集群内的事务投票。同时,ProposalRequestProcessor还会将事务请求交付给SyncRequestProcessor进行事务日志的记录。 SyncRequestProcessor,事务日志记录处理器。用来将事务请求记录到事务日志文件中,同时会触发Zookeeper进行数据快照。 AckRequestProcessor,负责在SyncRequestProcessor完成事务日志记录后,向Proposal的投票收集器发送ACK反馈,以通知投票收集器当前服务器已经完成了对该Proposal的事务日志记录。 CommitProcessor,事务提交处理器。对于非事务请求,该处理器会直接将其交付给下一级处理器处理;对于事务请求,其会等待集群内针对Proposal的投票直到该Proposal可被提交,利用CommitProcessor,每个服务器都可以很好地控制对事务请求的顺序处理。 ToBeCommitProcessor。该处理器有一个toBeApplied队列,用来存储那些已经被CommitProcessor处理过的可被提交的Proposal,其会将这些请求交付给FinalRequestProcessor处理器处理,待其处理完后,再将其从toBeApplied队列中移除。 FinalRequestProcessor,用来进行客户端请求返回之前的操作,包括创建客户端请求的响应,针对事务请求,该处理器还会负责将事务应用到内存数据库中。 2.FollowerFollower服务器是Zookeeper集群状态中的跟随者，其主要工作有以下三个： 处理客户端非事务性请求(读取数据) ,转发事务请求给Leader服务器。 参与事务请求Proposal的投票。 参与Leader选举投票。 和leader一样, Follower也采用了责任链模式组装的请求处理链来处理每一个客户端请求,由于不需要对事务请求的投票处理,因此Follower的请求处理链会相对简单,其处理链如下： 和Leader服务器的请求处理链最大的不同点在于, Follower服务器的第一个处理器换成了FollowerRequestProcessor处理器,同时由于不需要处理事务请求的投票,因此也没有了ProposalRequestProcessor处理器。 FollowerRequestProcessor：其用作识别当前请求是否是事务请求,若是,那么Follower就会将该请求转发给Leader服务器,Leader服务器在接收到这个事务请求后,就会将其提交到请求处理链,按照正常事务请求进行处理。 SendAckRequestProcessor：其承担了事务日志记录反馈的角色,在完成事务日志记录后,会向Leader服务器发送ACK消息以表明自身完成了事务日志的记录工作。 3.ObserverObserver充当了一个观察者的角色——其观察Zookeeper集群的最新状态变化并将这些状态变更同步过来。Observer服务器在工作原理上和Follower基本是一致的,对于非事务请求,都可以进行独立的处理,而对于事务请求,则会转发给Leader服务器进行处理。 和Follower唯一的区别在于, Observer不参与任何形式的投票,包括事务请求Proposal的投票和Leader选举投票。简单地讲, Observer服务器只提供非事务服务,通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力。另外, Observer的请求处理链路和Follower服务器也非常相近,其处理链如下： 需要注意的是,虽然在图中可以看到, Observer服务器在初始化阶段会将SyncRequestProcessor处理器也组装上去,但是在实际运行过程中, Leader服务器不会将事务请求的投票发送给Observer服务器。 三.服务器启动1.服务端整体架构图 Zookeeper服务器的启动，大致可以分为以下五个步骤： 1.配置文件解析 2.初始化数据管理器 3.初始化网络I&#x2F;O管理器 4.数据恢复 5.对外服务。 2.单机版服务器启动 上图过程可分为预启动和初始化过程。 2.1 预启动 统一由QuorumPeerMain作为启动类。无论单机或集群,在zkServer.cmd和zkServer.sh中都配置了QuorumPeerMain作为启动入口类。 解析配置文件zoo.cfg。zoo.cfg配置运行时的基本参数,如tickTime, dataDir,clientPort等参数。 创建并启动历史文件清理器DatadircleanupManager。对事务日志和快照数据文件进行定时清理。 判断当前是集群模式还是单机模式启动。若是单机模式,则委托给zooKeeperServerMain进行启动。 再次进行配置文件zoo.cfg的解析。 创建服务器实例ZooKeeperServer。zookeeper服务器首先会进行服务器实例的创建,然后对该服务器实例进行初始化,包括连接器、内存数据库、请求处理器等组件的初始化。 2.2 初始化 创建服务器统计器ServerStats。ServerStats是zookeeper服务器运行时的统计器。 创建Zookeeper数据管理器FilerxnSnapLog。FilerxnSnapLog是zookeeper上层服务器和底层数据存储之间的对接层,提供了一系列操作数据文件的接口,如事务日志文件和快照数据文件。Zookeeper根据zoo.cfq文件中解析出的快照数据目录dataDir和事务日志目录dataLogDir来创建FilerxnSnapLog。 设置服务器tickTime和会话超时时间限制。 创建ServerCnxnFactory。通过配置系统属性zookeper.serverCnxnFactory来指定使用zookeeper自己实现的NIO还是使用Netty框架作为zookeeper服务端网络连接工厂。 初始化ServerCnxnFactory。zookeeper会初始化Thread作为ServerCnxnFactory的主线程,然后再初始化NIO服务器。 启动ServerCnxnFactory主线程。进入Thread的run方法,此时服务端还不能处理客户端请求。 恢复本地数据。启动时,需要从本地快照数据文件和事务日志文件进行数据恢复。 创建并启动会话管理器。zookeeper会创建会话管理器SessionTracker进行会话管理。 初始化zookeeper的请求处理链。Zookeeper请求处理方式为责任链模式的实现。会有多个请求处理器依次处理一个客户端请求,在服务器启动时,会将这些请求处理器串联成一个请求处理链。 注册JMX服务。zookeeper会将服务器运行时的一些信息以JMX的方式暴露给外部。 注册zookeeper服务器实例。将zookeeper服务器实例注册给serverCnxnFactory,之后zookeeper就可以对外提供服务。 到此，单机版的Zookeeper服务器启动完毕。 3.集群服务器启动单机和集群服务器的启动在很多地方是一致的，流程图如下： 上图过程可以分为预启动、初始化、Leader选举、Leader与Follower启动期交互、Leader与Follower启动等过程。 3.1 预启动 统一由QuorumPeerMain作为启动类。 解析配置文件zoo.cfg。 创建并启动历史文件清理器DatadircleanupFactory。 判断当前是集群模式还是单机模式的启动。在集群模式中,在zoo.cfg文件中配置了多个服务器地址，可以选择集群启动。 3.2 初始化 创建ServerCnxnFactory。 初始化serverCnxnFactory。 创建zookeeper数据管理器FilerxnSnapLog。 创建QuorumPeer实例。Quorum是集群模式下特有的对象,是zookeeper服务器实例(zooKeeperServer)的托管者, QuorumPeer代表了集群中的一台机器,在运行期间,QuorumPeer会不断检测当前服务器实例的运行状态,同时根据情况发起Leader选举。 创建内存数据库ZKDatabase。ZKDatabase负责管理Zookeeper的所有会话记录以及Datarree和事务日志的存储。 初始化QuorumPeer。将核心组件如FileTxnSnapLog、ServerCnxnFactory,zKDatabase注册到QuorumPeer中,同时配置QuorumPeer的参数,如服务器列表地址、Leader选举算法和会话超时时间限制等。 恢复本地数据。 启动serverCnxnFactory主线程。 3.3 Leader选举1).初始化Leader选举。 集群模式特有, zookeeper首先会根据自身的服务器ID (sID) 、最新的ZxID (lastLoggedzxid)和当前的服务器epoch (currentEpoch)来生成一个初始化投票,在初始化过程中,每个服务器都会给自己投票。然后,根据zoo.cfg的配置,创建相应Leader选举算法实现, zookeeper提供了三种默认算法(LeaderElection,AuthFastLeaderElection、FastLeaderElection) ,可通过zoo.cfg中的electionAlg属性来指定,但现只支持FastLeaderElection选举算法。在初始化阶段, zookeeper会创建Leader选举所需的网络I&#x2F;O层QuorumCnxManager,同时启动对Leader选举端口的监听,等待集群中其他服务器创建连接。 2).注册JMX服务。 3).检测当前服务器状态。 运行期间, QuorumPeer会不断检测当前服务器状态。在正常情况下, zookeeper服务器的状态在LOOKING、 LEADING、FOLLOWING&#x2F;OBSERVING之间进行切换。在启动阶段, QuorumPeer的初始状态是LOOKING,因此开始进行Leader选举。 4).Leader选举。 Zookeeper的Leader选举过程,简单地讲,就是一个集群中所有的机器相互之间进行一系列投票,选举产生最合适的机器成为Leader,同时其余机器成为Follower或是observer的集群机器角色初始化过程。关于Leader选举算法,简而言之,就是集群中哪个机器处理的数据越新(通常我们根据每个服务器处理过的最大ZxID来比较确定其数据是否更新) ,其越有可能成为Leader。当然,如果集群中的所有机器处理的zxID-致的话,那么sID最大的服务器成为Leader,其余机器称为Follower和observer。 3.4 Leader与Follower启动期交互到这里为止, ZooKeeper已经完成了Leader选举,并且集群中每个服务器都已经确定了自己的角色——通常情况下就分为Leader和Follower两种角色。下面我们来对Leader和Follower在启动期间的交互进行介绍,其大致交互流程如图所示。 创建Leader服务器和Follower服务器。完成Leader选举后,每个服务器会根据自己服务器的角色创建相应的服务器实例,并进入各自角色的主流程。 Leader服务器启动Follower接收器LearnerCnxAcceptor。运行期间, Leader服务器需要和所有其余的服务器(统称为Learner)保持连接以确集群的机器存活情况, LearnerCnxAcceptor负责接收所有非Leader服务器的连接请求。 Learner服务器开始和Leader建立连接。所有Learner会找到Leader服务器,并与其建立连接。 Leader服务器创建LearnerHandler, Leader接收到来自其他机器连接创建请求后,会创建一个earnerHandler实例,每个LearnerHandler实例都对应一个Leader与Learner服务器之间的连接,其负责Leader和Learner服务器之间几乎所有的消息通信和数据同步。 向Leader注册。Learner完成和Leader的连接后,会向Leader进行注册,即将Learner服务器的基本信息(Learnerlnfo) ,包括SID和ZXID,发送Leader服务器。 Leader解析Learner信息，计算新的epoch。Leader接收到Learner服务器基本信息后,会解析出该Learner的SID和ZXID,然后根据ZXID解析出对应的epoch_oflearner,并和当前Leader服务器的epoch_ofleader进行比较,如果该Learner的epoch_oflearner更大,则更新Leader的epoch_of leader &#x3D; epoch_oflearner +1,然后LearnHandler进行等待,直到过半Learner已经句Leader进行了注册,同时更新了epoch_ofleader后, Leader就可以确定当前集群的epoch了。 发送Leader状态。计算出新的epoch后, Leader会将该信息以一个LEADERINFO消息的形式发送给Learner,并等待Learner的响应。 Learner发送ACK消息。Learner接收到LEADERINFO后,会解析出epoch和ZXID,然后向Leader反馈一个ACKEPOCH响应。 数据同步。Leader收到Learner的ACKEPOCH后,即可进行数据同步。 启动Leader和Learner服务器。当有过半Learner已经完成了数据同步,那么Leader和Learner服务器实例就可以启动了。 3.5 Leader与Follower启动 创建启动会话管理器。 初始化Zookeeper请求处理链，集群模式的每个处理器也会在启动阶段串联请求处理链。 注册JMX服务。 至此，集群版的Zookeeper服务器启动完毕。 四.leader选举当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。 服务器初始化启动。 服务器运行期间无法和Leader保持连接。 1.服务器启动时期的Leader选举若进行Leader选举,则至少需要两台机器,这里选取3台机器组成的服务器集群为例。在集群初始化阶段,当有一台服务器Server1启动时,其单独无法进行和完成Leader选,当第二台服务器Server2启动时,此时两台机器可以相互通信,每台机器都试图找到Leader,于是进入Leader选举过程。选举过程如下： 1).每个Server发出一个投票 由于是初始情况, Server1 (假设myid为1)和Server2假设myid为2)都会将自己作为Leader服务器来·进行投票,每次投票会包含所推举的服务器的myid和ZXID,使用(myid, ZXID)来表示,此时Server1的投票为(1, 0), Server2的投票为(2, 0),然后各自将这个投票发给集群中其他机器。 2).接受来自各个服务器的投票 集群的每个服务器收到投票后,首先判断该投票的有效性,如检查是否是本轮投票、是否来自LOOKING状态的服务器。 3).处理投票 针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下： 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 如果ZXID相同,那么就比较myid。myid较大的服务器作为Leader服务器。 现在我们来看Server1和Server2实际是如何进行投票处理的。对于Server1来说,它自己的投票是(1, 0) ,而接收到的投票为(2, 0) 。首先会对比两者的ZXID,因为都是0,所以无法决定谁是Leader,接下来会对比两者的myid,很显然, Server1发现接收到的投票中的myid是2,大于自己,于是就会更新自己的投票为(2, 0) ,然后重新将投票发出去。而对于Server2来说,不需要更新自己的投票。 4).统计投票 每次投票后,服务器都会统计所有投票,判断是否已经有过半的机器接收到相同的投票信息。对于Server1和server2服务器来说,都统计出集群中已经有两台机器接受了(2, 0)这个投票信息。这里我们需要对”过半”的概念做一个简单的介绍。所谓”过半”就大于集群机器数一半,即大于或等于(n&#x2F;2+1) 。对于这里由3台机器构成的集群,大于等于2台即为达到”过半”要求。 即当Server1和Server2都收到相同的投票信息（2，0）的时候，认为已经选出了Leader。 5).改变服务器状态 一旦确定了Leader,每个服务器就会更新自己的状态：如果是Follower,那么就变更为FOLLOWING,如果是Leader,那么就变更为LEADING。 2.服务器运行时期的Leader选举在ZooKeeper集群正常运行过程中,一旦选出一个Leader,那么所有服务器的集群角色一般不会再发生变化——也就是说, Leader服务器将一直作为集群的Leader,即使集群中有非Leader机器挂了或是有新机器加入集群也不会影响Leader。但是一旦Leader所在的机器挂了,那么整个集群将暂时无法对外服务,而是进入新一轮的Leader选举。服务器运行期间的Leader选举和启动时期的Leader选举基本过程是一致的。 还是假设当前正在运行的ZooKeeper机器由3台机器组成,分别是Server1、Server2和Server3, 当前的Leader是Server2。假设在某一个瞬间, Leader挂了,这个时候便开始了Leader选举。 1).变更状态 Leader挂后,余下的非Observer服务器都会将自己的服务器状态变更为LOOKING,然后开始进入Leader选举过程。 2).每个Server会发出一个投票 在运行期间,每个服务器上的ZXID可能不同,此时假定Server1的ZXID为123, Server3的ZXID为122;在第一轮投票中, Server1和Server3都会投自己,产生投票(1, 123), (3, 122),然后各自将投票发送给集群中所有机器。 3).接收来自各个服务器的投票,与启动时过程相同。 4).处理投票。与启动时过程相同,此时,Server1将会成为Leader。 5).统计投票。与启动时过程相同。 6).改变服务器的状态。与启动时过程相同。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.flamingo-sky.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://www.flamingo-sky.com/tags/zookeeper/"}]},{"title":"zookeeper基本概念和使用","slug":"zookeeper基本概念和使用","date":"2022-02-14T21:30:43.000Z","updated":"2022-02-14T22:56:03.613Z","comments":true,"path":"2022/02/15/zookeeper基本概念和使用/","link":"","permalink":"https://www.flamingo-sky.com/2022/02/15/zookeeper%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"","text":"zookeeper基本概念和使用一. 基础概念集群角色 leader follower observer 会话 客户端和服务端之间连接的一个TCP长连接，zookeeper对外的服务端口默认是2181. 数据节点 机器节点：构成集群的机器 数据节点：指数据模型中的数据单元 版本Zookeeper会为每个ZNode维护一个Stat的数据结构，Stat记录了这个ZNode的三个数据版本，分别是version（当前ZNode版本）、cversion（当前ZNode子节点版本）、aversion（当前ZNode的ACL版本）。 事件监听器（watcher）在指定节点上注册一些Watcher，这些事件监听器触发时，Zookeeper服务端会将事件通知给感兴趣的客户端，这机制是Zookeeper实现分布式协调服务的重要特性。 ACLACL（Access Control Lists）权限控制策略。如下五种权限： CREATE：创建子节点的权限 READ：获取节点数据和子节点列表的权限 WRITE：更新节点数据的权限 DELETE：删除子节点的权限 ADMIN：设置节点ACL的权限 二. 基本使用1. zookeeper系统模型在Zookeeper中数据信息被保存在一个个数据节点上,这些节点被称为ZNode。ZNode是zookeeper中最小数据单位，类似文件系统的层级树状结构。 1.1 ZNode的类型数据节点ZNode的节点类型分类：持久性节点（Persistent）、临时性节点（Ephemeral）、顺序性节点（Sequential）。 创建节点时可以通过组合生成以下四种节点类型：持久节点、持久顺序节点、临时节点、临时顺序节点。不同类型的节点会有不同的生命周期。 持久节点：节点被创建后会一直存在服务器，知道删除操作主动清除 持久顺序节点：创建节点时会在节点名后面加上一个数字后缀来表示顺序，和持久节点特性一样 临时节点：生命周期和客户端会话绑在一起，客户端会话结束，节点会被删除；不能创建子节点 临时顺序节点：有顺序的临时节点 在zookeeper中，事务是指能够改变zookeeper服务器状态的操作，称为事务操作和更新操作。一般包括数据节点的创建与删除、数据节点内容更新等操作。 每一次事务请求，zk都会为其分配一个全局唯一的事务id，用ZXID表示，通常是一个64位数字。每一个ZXID对应一次更新更新操作。 1.2 ZNode的状态信息 整个ZNode节点内容包括两部分：节点数据内容和节点状态信息。quota是数据内容，其它属于状态信息。 cZxid 就是 Create ZXID，表示节点被创建时的事务ID。 ctime 就是 Create Time，表示节点创建时间。 mZxid 就是 Modified ZXID，表示节点最后⼀次被修改时的事务ID。 mtime 就是 Modified Time，表示节点最后⼀次被修改的时间。 pZxid 表示该节点的⼦节点列表最后⼀次被修改时的事务 ID。只有⼦节点列表变更才会更新 pZxid，⼦节点内容变更不会更新。 cversion 表示⼦节点的版本号。 dataVersion 表示内容版本号。 aclVersion 标识acl版本。 ephemeralOwner 表示创建该临时节点时的会话 sessionID，如果是持久性节点那么值为 0。 dataLength 表示数据⻓度。 numChildren 表示直系⼦节点数。 1.3 Watcher数据变更通知 Zookeeper使用Watcher机制实现分布式数据的发布&#x2F;订阅功能。多个订阅者同时监听某一个主题对象，主题对象状态发生变化时，会通知所有的订阅者做出相应处理。 Zookeeper允许客户端向服务端注册一个Watcher监听，当服务端的指定事件触发了Watcher，就会向指定客服端发送一个事件通知。 Zookeeper的Watcher机制包括客户端线程、客服端WatcherManager、Zookeeper服务器三部分。 具体流程：客户端向zk服务器注册的同时会将Watcher对象存储在客户端的WatcherManager中，当Zookeeper服务器触发Watcher使劲按后回向客服端发送通知，客户端线程从WatcherManager中取出对应的Watcher对象来执行回调逻辑。 1.4 ACL保障数据的安全在Zookeeper中提供了一套完善的ACL（Access Control List）权限控制机制来保障数据的安全。 通常会使用“权限模式（scheme）：授权对象（id）：权限（permission）”来标志一个有效的ACL信息。 权限模式用来确定权限验证中使用的检验策略 授权对象指的是权限赋予的用户或一个指定实体 权限指通过权限检查后可以被允许执行的操作 权限与授权对象的关系： 权限模式 授权对象 IP 通常使用IP地址或IP段。例如：192.168.91.105（IP）或 192.168.91.1&#x2F;24（网段） Digest 自定义，通常是username：BASE64（SHA-1（username：password））进行加密再编码 World 只有一个ID：anyone Super 超级用户 权限分为五大类：CREATE（子节点）、DELETE（子节点）、READ、WRITE、ADMIN（对节点进行ACL设置）。简称为CDRWA。 2.命令行操作对节点的增删改查常用命令：输入help后，回显示可用的Zookeeper命令 create 【-s】【-e】path data acl：创建节点（顺序&#x2F;临时） &#x2F;zlg 123 ls path：显示节点的所有直系子节点 get path：显示节点的内容和属性信息 ls2 path：显示节点的直系子节点列表和属性信息 set path data 【version】：更新指定节点的数据内容，data表示更新的内容，version表示数据版本 delete path 【version】：删除指定的节点，version表示数据版本（dataVersion），若删除节点存在子节点，就无法删除该节点，必须先删除子节点，再删除父节点 3.相关客户端api使用有Zookeeper的原生API、ZkClient、Curator 三种使用方式。下面对这三种使用方式的API做下对比： 引入依赖： 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.14&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 原生部分API： 创建会话：new Zookeeper(connectString,sesssionTimeOut,Watcher) 创建节点：zookeeper.create(path,data,acl,createMode) 删除节点：zookeeper.delete(path,version) 获取数据：zk.getData(path, watch, stat) 获取子节点列表：zooKeeper.getChildren(path, watch) 更新数据：zooKeeper.setData(path, data,version) ZkClient客户端： 创建会话：new ZkClient(serverString) 创建节点：zkClient.createPersistent(String path, boolean createParents) 删除节点：zkClient.deleteRecursive(String path) 获取数据：zkClient.readData(path) 获取子节点列表：zkClient.getChildren(path) 更新数据：zkClient.writeData(path, object) 事件监听： zkClient.subscribeChildChanges(path, new IZkChildListener() {…}) zkClient.subscribeDataChanges(path, new IZkDataListener() {…} Curator客户端： 创建会话：public static CuratorFramework newClient(String connectString, int sessionTimeoutMs, int connectionTimeoutMs, RetryPolicy retryPolicy) 创建节点：client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPa th(path,data); data需要序列化 data &#x3D; serializer.serialize(new ServerInfo(ip, port, 0L)) 删除节点：client.delete().deletingChildrenIfNeeded().forPath(path) 获取数据：client.getData().forPath(path)得到的字节数组bytes需要反序列化; serializer.deserialize(ServerInfo.class, bytes); 获取子节点列表：client.getChildren().forPath(path) 更新数据：client.setData().forPath(path, data) 三.zookeeper应用场景利用 ZooKeeper 可以非常方便构建一系列分布式应用中都会涉及到的核心功能。 数据发布&#x2F;订阅 负载均衡 命名服务 分布式协调&#x2F;通知 集群管理 Master 选举 分布式锁 分布式队列 多个开源项目中都应用到了 ZooKeeper，例如 HBase, Spark, Flink, Storm, Kafka, Dubbo 等等。 Zookeeper应用场景文章推荐： ZooKeeper 的应用场景(opens new window) 图解ZooKeeper的典型应用场景(opens new window) ZooKeeper应用场景及方案介绍(opens new window) Zookeeper系列（6）– Zookeeper的典型应用场景(opens new window) Zookeeper应用场景(opens new window) 下面主要看下Zookeeper在分布式锁上的应用场景 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源,那么访问这些资源的时候,往往需要通过一些互斥手段来防止彼此之间的干扰,以保证一致性,在这种情况下,就需要使用分布式锁了。下面看下Zookeeper如何实现排他锁和共享锁两类分布式锁。 1.排他锁排他锁(Exclusive Locks,简称×锁) ,又称为写锁或独占锁,是一种基本的锁类型。如果事务T1对数据对象o1加上了排他锁,那么在整个加锁期间,只允许事务T1对o1进行读取和更新操作,其他任何事务都不能再对这个数据对象进行任何类型的操作——直到T1释放了排他锁。 排他锁的核心是如何保证当前有且仅有一个事务获,得锁,并且锁被释放后,所有正在等待获取锁的事务都能够被通知到。 Zookeeper实现排他锁步骤如下： 1.定义锁 Java并发编程中可以使用synchronized机制和ReentrantLock来定义锁。Zookeeper中需要定义一个临时节点来表示一个锁，如&#x2F;exclusive_lock&#x2F;lock节点可以被定义为一个锁。如图： 2.获取锁 在需要获取排他锁时，所有客户端都试图在&#x2F;exclusive_lock节点下通过调用create()接口创建临时子节点。谁创建成功了就表示谁获取了排他锁，同时所有没有获取到锁的客户端就需要在&#x2F;exclusive_lock节点上注册一个子节点变更的Watcher监听，来监听子节点的变化情况。 3.释放锁 当获取锁的客户端发生宕机或正常执行完业务逻辑后，临时节点都会被移出，也就释放了锁。Zookeeper就会通知所有在&#x2F;exclusive_lock节点上注册监听的客户端，客户端接收到通知后再次发起分布式锁获取。 2.共享锁共享锁(Shared Locks,简称S锁) ,又称为读锁,同样是一种基本的锁类型。如果事务T1对数据对象o1加上了共享锁,那么当前事务只能对o1进行读取操作,其他事务也只能对这个数据对象加共享锁——直到该数据对象上的所有共享锁都被释放。 共享锁和排他锁最根本的区别在于：加上排他锁后,数据对象只对一个事务可见,而加上共享锁后,数据对所有事务都可见。 Zookeeper实现共享锁步骤如下： 1.定义锁 Zookeeper上通过定义一个临时顺序节点来代表一个共享锁。如&#x2F;share_lock&#x2F;host1-R-0000000001，如图所示： 2.获取锁 在需要获取共享锁时,所有客户端都会到&#x2F;shared lock这个节点下面创建一个临时顺序节点,如果当前是读请求,那么就创建例如&#x2F;shared lock&#x2F;host1-R-000000000节点;如果是写请求,那么就创建例如&#x2F;sharedlock&#x2F;host2-W-0000000002的节点。 判断读写顺序：通过Zookeeper来去确定分布式读写顺序，大致分为四步。 1,创建完节点后,获取&#x2F;shared-lock节点下所有子节点,并对该节点变更注册监听。 2,确定自己的节点序号在所有子节点中的顺序。 3,对于读请求:若没有比自己序号小的子节点或所有比自己序号小的子节点都是读请求,那么表明自己已经成功获取到共享锁,同时开始执行读取逻辑,若有写请求,则需要等待。对于写请求:若自己不是序号最小的子节点,那么需要等待。 4,接收到Watcher通知后,重复步骤1 3.释放锁，流程与排他锁一致。 3.羊群效应上面的共享锁的实现大体能满足集群规模不是特别大的场景。当规模扩大之后，会出现什么问题呢？结合下图看下实际运行情况： 可以看到，host1客户端移出自己的共享锁后，只对host2产生影响了，对其它机器没有任何作用。大量的Watcher通知和子节点列表获取两个操作会重复运行,这样不仅会对zookeeper服务器造成巨大的性能影响影响和网络开销,更为严重的是,如果同一时间有多个节点对应的客户端完成事务或是事务中断引起节点消失, ZooKeeper服务器就会在短时间内向其余客户端发送大量的事件通知,这就是所谓的羊群效应。 上面的分布式锁竞争过程,它的核心逻辑在于:判断自己是否是所有子节点中序号最小的。于时可以联想到每个节点对应的客户端只需要关注比自己序号小的那个相关节点的变更情况就可以了–而不需要关注全局的子列表变更情况。 4.改进后的分布式锁实现上面提到的共享锁实现,从整体思路上来说完全正确。这里主要的改动在于:每个锁竞争者,只需要关注&#x2F;sharedlock节点下序号比自己小的那个节点是否存在即可,具体实现如下。 客户端调用create接口常见类似于&#x2F;shared_lock&#x2F;[Hostname]-请求类型-序号的临时顺序节点。 客户端调用getChildren接口获取所有已经创建的子节点列表(不注册任何Watcher)。 如果无法获取共享锁,就调用exist接口来对比自己小的节点注册Watcher,对于读请求:向比自己序号小的最后一个写请求节点注册Watcher监听。对于写请求:向比自己序号小的最后一个节点注册Watcher监听。 等待Watcher通知,继续进入步骤2。 如同在多线程并发编程实践中,我们会去尽量缩小锁的范围——对于分布式锁实现的改进其实也是同样的思路。根据具体的业务场景和集群规模来选择适合自己的分布式锁实现:在集群规模不大、网络资源丰富的情况下,第一种分布式锁实现方式是简单实用的选择;而如果集群规模达到一定程度,并且希望能够精细化地控制分布式锁机制,那么就可以试试改进版的分布式锁实现。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.flamingo-sky.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://www.flamingo-sky.com/tags/zookeeper/"}]},{"title":"如何使用 Github + Hexo 搭建博客","slug":"如何使用 Github + Hexo 搭建博客","date":"2021-03-13T01:47:57.000Z","updated":"2022-02-12T19:57:36.693Z","comments":true,"path":"2021/03/13/如何使用 Github + Hexo 搭建博客/","link":"","permalink":"https://www.flamingo-sky.com/2021/03/13/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20Github%20+%20Hexo%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","excerpt":"搭建初衷：记录自己的学习笔记。 Github Pages 上搭建博客的弊端： 部署在 Github 上，需要科学上网 整个流程：撰写 md 文档 ——&gt; Hexo 生成静态网页 ——&gt; 部署到Github Pages ——&gt; 浏览 维护一个静态网站，需要有容灾和备份的意识和能力。","text":"搭建初衷：记录自己的学习笔记。 Github Pages 上搭建博客的弊端： 部署在 Github 上，需要科学上网 整个流程：撰写 md 文档 ——&gt; Hexo 生成静态网页 ——&gt; 部署到Github Pages ——&gt; 浏览 维护一个静态网站，需要有容灾和备份的意识和能力。 搭建博客1. 环境搭建Hexo 基于 Node.js，搭建过程中还需要使用 npm（Node.js 已带） 和 git，因此先搭建本地操作环境，安装 Node.js 和 Git。 Node.js：https://nodejs.org/zh-cn Git：https://git-scm.com/downloads 下载 Node.js 和 Git 程序并安装，一路点 “下一步” 按默认配置完成安装。 安装完成后，Win+R 输入 cmd 并打开，依次输入 node -v、npm -v 和 git –version 并回车，如下图出现程序版本号即可 2. 连接博客使用邮箱注册 GitHub 账户，选择免费账户（Free），并完成邮件验证。 右键 -&gt; Git Bash Here，设置用户名和邮箱： git config –global user.name “GitHub 用户名”git config –global user.email “GitHub 邮箱” 创建 SSH 密匙 输入 ssh-keygen -t rsa -C “GitHub 邮箱”，然后一路回车。 添加密匙： 进入 [C:\\Users\\用户名.ssh] 目录（要显示隐藏文件，如果路径不对可以看上一步 创建 SSH 密钥时的输出内容，会有路径信息），打开公钥 id_rsa.pub 文件并复制里面的内容。 登陆 GitHub ，进入 Settings 页面，选择左边栏的 SSH and GPG keys，点击 New SSH key。 Title 随便取个名字，粘贴复制的 id_rsa.pub 内容到 Key 中，点击 Add SSH key 完成添加。 验证连接： 打开 Git Bash，输入 ssh -T &#103;&#x69;&#116;&#64;&#103;&#x69;&#116;&#x68;&#x75;&#x62;&#x2e;&#99;&#111;&#x6d; 出现 “Are you sure……”，输入 yes 回车确认 显示 “Hi xxx! You’ve successfully……” 即连接成功。 3. 创建 Github pages 仓库GitHub 主页右上角加号 -&gt; New repository： Repository name 中输入：用户名.github.io 勾选 Add a README file，会自动设置分支（分支名设置成master）：This will set master as the default branch. create repository 4. 创建保存源码的分支GitHub Pages 会自动部署静态网页文件，并将 master 分支作为部署的默认分支。为将静态网页和源文件（包含文章、主题等）分离开，强烈建议创建新分支，这样 master 分支只用来发布静态网页，而文档编辑和 Hexo 操作都在另一个分支上完成。 打开博客所在本地的目录，右键 -&gt; Git Bash Here，将 git 仓库 clone 至本地： git clone https://github.com/用户名/用户名.github.io.git cd 命令进入仓库目录,再创建本地分支： git checkout -b hexo-source # hexo-source 是我的分支名 切换到新建分支(hexo-source是我的分支名) git checkout -b hexo-source 查看本地分支(windows),此时 hexo-source 分支应该是高亮(当前分支) git branch -l 5. 本地安装 Hexo 博客程序由于只能在空文件夹中生成 Hexo 项目,所以我们先将 .git 以及其他文件(如 README.MD)移出去,完成初始化后再移回来. ** 安装 Hexo ** npm install -g hexo-cli Hexo 初始化和本地预览初始化并安装所需组件： hexo init # 初始化npm install # 安装组件 启动本地服务器及进行预览 hexo g # 生成页面hexo s # 启动预览 访问 http://localhost:4000， 出现 Hexo 默认页面，本地博客安装成功！ 6. 部署 Hexo 到 Github Pages本地博客测试成功后，就是上传到 GitHub 进行部署，使其能够在网络上访问。 首先安装 hexo-deployer-git： npm install hexo-deployer-git –save 然后修改 _config.yml 文件末尾的 Deployment 部分，修改成如下： 1234deploy: type: git repo: git@github.com:用户名/用户名.github.io.git branch: master 执行 hexo g -d部署静态页面至 Github Pages. 如果成功,此时通过https://yonghuming.github.io/ 会出现 Hexo 默认页面. 7. 部署 源文件到 Github Pages 突然发现 github pages 仓库中没有 hexo-source 分支,才想起还没有将本地 git pull 到 github 上,按理说此步骤可以提前至创建分支那一步. 先查看本地分支和远程仓库分支,发现本地和远程不一致,本地存在我创建的 hexo-source 分支 git branch -a 将本地创建的分支 push 到 github 仓库,两个 hexo-source,一个是本地名,一个是远程仓库里的命名. git push origin hexo-source:hexo-source 由于有部分是 Hexo 初始化的文件,不需要上传,可以过滤掉.打开 .gitignore文件,选择 过滤的文件 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 将本地的源文件 push 到 github 仓库. 123git add .git commit -m &#x27;hexo source post&#x27; # 引号内是描述git push origin hexo-source # hexo-source 是分支名 注意：如果是通过 git clone 下载配置的主题， push 源文件时需要将主题的 .git文件夹删除或改名备份。 8. 更换主题在 Themes | Hexo 选择一个喜欢的主题，比如 NexT，进入网站目录打开 Git Bash Here 下载主题： 主题链接: https://hexo.io/themes/我所用的 NexT主题说明文档: http://theme-next.iissnan.com/getting-started.html 9. 发布文章进入博客所在目录，右键打开 Git Bash Here，创建博文： hexo new “My New Post” 然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。 写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。以后每次发布文章都是这两条命令。 hexo g # 生成页面hexo d # 部署发布 也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 hexo g 和 hexo d 发布。 12345678910111213---title: Hello World # 标题date: 2019/3/26 hh:mm:ss # 时间categories: # 分类- Diarytags: # 标签- PS3- Games---摘要&lt;!--more--&gt;正文 10. 常见命令12345678hexo new &quot;name&quot; # 新建文章hexo new page &quot;name&quot; # 新建页面hexo g # 生成页面hexo d # 部署hexo g -d # 生成页面并部署hexo s # 本地预览hexo clean # 清除缓存和已生成的静态文件hexo help # 帮助 参考 使用 Hexo+GitHub 搭建个人免费博客教程（小白向） s1mplecc 的 ghost-to-hexo-migrater 项目 使用git分支保存hexo博客源码到github","categories":[{"name":"网站建设","slug":"网站建设","permalink":"https://www.flamingo-sky.com/categories/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE/"},{"name":"Hexo 博客","slug":"网站建设/Hexo-博客","permalink":"https://www.flamingo-sky.com/categories/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE/Hexo-%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"Hexo 博客","slug":"Hexo-博客","permalink":"https://www.flamingo-sky.com/tags/Hexo-%E5%8D%9A%E5%AE%A2/"},{"name":"搭建环境","slug":"搭建环境","permalink":"https://www.flamingo-sky.com/tags/%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/"}]}],"categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.flamingo-sky.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"网站建设","slug":"网站建设","permalink":"https://www.flamingo-sky.com/categories/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE/"},{"name":"Hexo 博客","slug":"网站建设/Hexo-博客","permalink":"https://www.flamingo-sky.com/categories/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE/Hexo-%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://www.flamingo-sky.com/tags/zookeeper/"},{"name":"Hexo 博客","slug":"Hexo-博客","permalink":"https://www.flamingo-sky.com/tags/Hexo-%E5%8D%9A%E5%AE%A2/"},{"name":"搭建环境","slug":"搭建环境","permalink":"https://www.flamingo-sky.com/tags/%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/"}]}